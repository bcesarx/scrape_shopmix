{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.18-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\bcesa\\anaconda3\\envs\\market_scrapers\\lib\\site-packages (from sqlalchemy) (4.7.1)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy)\n",
      "  Downloading greenlet-2.0.2-cp310-cp310-win_amd64.whl (192 kB)\n",
      "     ------------------------------------- 192.2/192.2 kB 11.4 MB/s eta 0:00:00\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-2.0.2 sqlalchemy-2.0.18\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibiotecas\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "###\n",
    "from src import config\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from src import dw_utils as dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bcesa\\anaconda3\\envs\\market_scrapers\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### atualzar lista de links no sitemap\n",
    "BASE_URL = 'https://www.gruposhopmix.com/'\n",
    "## obter lista de urls a partir do index\n",
    "\n",
    "content = requests.get(config.URL_INDEX).text\n",
    "\n",
    "### processar o html obtido \n",
    "### extracts all loc tags from content\n",
    "soup = bs(content, 'html.parser')\n",
    "all_locs = soup.find_all('loc')\n",
    "urls = pd.DataFrame(columns = [\"links_catalogo\"], data=all_locs)\n",
    "\n",
    "### remove keywords from urls\n",
    "keywords = ['pagina','sao-paulo', 'todos-os-produtos', 'os-mais-vendidos', 'novidades']\n",
    "urls = urls[~urls[\"links_catalogo\"].str.contains('|'.join(keywords))]\n",
    "## drop lines that is equal to the base url\n",
    "urls = urls.loc[urls[\"links_catalogo\"] != BASE_URL]\n",
    "### obter dataset final com urls\n",
    "urls.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_data_from_html(soup):\n",
    "    \n",
    "    \n",
    "    nome = soup.find('meta', property='og:title')['content']\n",
    "    codigo = soup.find('meta', attrs={'name': 'twitter:data1'})['content']\n",
    "    try:\n",
    "        preco = soup.find('strong', attrs = 'preco-promocional cor-principal titulo')['data-sell-price']\n",
    "    except TypeError:\n",
    "        preco = \"N/A\"\n",
    "    disponibilidade = soup.find('meta', attrs={'name': 'twitter:data2'})['content']\n",
    "    descricao = soup.find('meta', attrs={'name': 'description'})['content']\n",
    "    try:\n",
    "        disponibilidade_entrega = soup.find('span', class_='disponibilidade disp-entrega').find('b').text\n",
    "    except AttributeError:\n",
    "        disponibilidade_entrega = 'Não disponível'\n",
    "    try:\n",
    "        estoque = soup.find('b', class_='qtde_estoque').text\n",
    "    except AttributeError:\n",
    "        estoque = 'Não disponível'\n",
    "    imagens = [img['src'] for img in soup.find_all('img') if 'produto' in img['src']]\n",
    "    \n",
    "    return {\n",
    "        'nome': nome,\n",
    "        'código': codigo,\n",
    "        'preço': preco,\n",
    "        'descrição': descricao,\n",
    "        'disponibilidade': disponibilidade,\n",
    "        'disponibilidade_entrega': disponibilidade_entrega,\n",
    "        'estoque': estoque,\n",
    "        'imagens': imagens\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Obtendo dados dos produtos: 100%|██████████| 1511/1511 [2:57:25<00:00,  7.05s/produto]  \n"
     ]
    }
   ],
   "source": [
    "# obter soup de cada url de cada produto\n",
    "\n",
    "\n",
    "df_produtos_consolidado = pd.DataFrame()\n",
    "counter = 0\n",
    "for link in tqdm(urls[\"links_catalogo\"], desc='Obtendo dados dos produtos', unit='produto'):\n",
    "\n",
    "    ## baixar pagina do produto\n",
    "    prod_content= requests.get(link).text\n",
    "    ### converter página em soup\n",
    "    prod_content_soup = bs(prod_content, 'html.parser')\n",
    "    ### extrair dicionário com os dados chave do produto ## converter dicionário em dataframe\n",
    "    dict_produto = extract_data_from_html(prod_content_soup)\n",
    "    ## converter a lita de imagens em str\n",
    "    dict_produto[\"imagens\"] = str(dict_produto[\"imagens\"])\n",
    "\n",
    "    ##criar um dataframe do produto\n",
    "    df_produto = pd.DataFrame(dict_produto, index=[0])\n",
    "    ###inserir o link do produto\n",
    "    df_produto[\"link\"] = link\n",
    "    ### inseirr a data de extração\n",
    "    df_produto[\"data_extracao\"] = datetime.now()\n",
    "    ## adicionar ao dataframe consolidado\n",
    "    df_produtos_consolidado = pd.concat([df_produtos_consolidado, df_produto], ignore_index=True)\n",
    "    ##colocar uma pausa de 1 segundo a cada 50 produtos\n",
    "    counter += 1\n",
    "    time.sleep(5)\n",
    "    if counter  == 201:\n",
    "        time.sleep(57)\n",
    "        counter = 0\n",
    "        \n",
    "\n",
    "# salvar dataframe consolidado em csv\n",
    "\n",
    "df_produtos_consolidado.to_csv('data/produtos_consolidado.csv', index=False, sep='|')\n",
    "\n",
    "### salvar dataframe consolidado no banco de dados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index\n",
      "\u001b[1;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server connected via SSH || Local Port: 63752...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p_host = 'localhost'\n",
    "p_port = 5432\n",
    "db = 'postgres'\n",
    "ssh = True\n",
    "ssh_user = 'ubuntu'\n",
    "ssh_host = '144.22.150.9'\n",
    "psql_user = 'postgres'\n",
    "\n",
    "\n",
    "psql_pass = 'alice11'\n",
    "ssh_pkey = r\"C:\\Users\\bcesa\\OneDrive\\Documentos\\Infra na Núvem\\mydata\\ssh-key-2022-10-28.key\"\n",
    "\n",
    "pgres = dw.Postgresql_connect(pgres_host=p_host, pgres_port=p_port, db=db, ssh=ssh, ssh_user=ssh_user, ssh_host=ssh_host, ssh_pkey=ssh_pkey, psql_user=psql_user\n",
    "                              , psql_pass=psql_pass)\n",
    "#initiates a connection to the PostgreSQL database. In this instance we use ssh and must specify our ssh credentials.\n",
    "\n",
    "#You'll need to define psql_user and psql_pass using input() and getpass() to temporarily store your credentials.\n",
    "#Alternatively, best practice you may be to store your credentials as environment variables.\n",
    "# psql_user = input(\"Please enter your database username:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_produtos_consolidado = pd.read_csv('data/produtos_consolidado.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database [postgres] session created...\n",
      "<> Table Replaced <>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 08:25:23,878| ERROR   | Socket exception: Foi forçado o cancelamento de uma conexão existente pelo host remoto (10054)\n"
     ]
    }
   ],
   "source": [
    "### criar conexão com o banco de dados\n",
    "\n",
    "pgres.replace_table(df_produtos_consolidado, 'catalogo_shopmix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.6-cp310-cp310-win_amd64.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.6\n"
     ]
    }
   ],
   "source": [
    "! pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database [postgres] session created...\n",
      "<> Query Sucessful <>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [link]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 14:13:30,268| ERROR   | Socket exception: Foi forçado o cancelamento de uma conexão existente pelo host remoto (10054)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sql_statement = \"\"\"\n",
    "SELECT *\n",
    "FROM catalogo_shopmix\n",
    ";\n",
    "\"\"\"\n",
    "query_df = pgres.query( query=sql_statement)\n",
    "query_df\n",
    "#returns the results of an sql statement as a pandas dataframe. \n",
    "#This example returns the column names and data types of table 'ey_test_table'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shopmix_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
